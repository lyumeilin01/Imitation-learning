{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 579 - A3\n",
    "Meilin Lyu 260899756\n",
    "Alexander Aleshchenko 260923564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pandas.read_csv(file_path, index_col=False)\n",
    "    return data\n",
    "\n",
    "np.set_printoptions(edgeitems=30)\n",
    "# expert_100 = np.genfromtxt('expert_100.csv', delimiter=',')[1:-1]\n",
    "# expert_100_im = load_data('expert_100.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Discretization and One-Hot Encoding helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dimension of state: num_data by 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(s1, s2, s3, s4):\n",
    "    a = np.array([s1, s2, s3, s4])\n",
    "    b = np.zeros((a.size, 10))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b.reshape(-1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def getBin(s, intervals):\n",
    "    for i in range(len(intervals)):\n",
    "        if intervals[i].left <= s <= intervals[i].right:\n",
    "            return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Simple Imitation learning - using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_imitation(data):\n",
    "    data_ac = data[[\"state_0\", \"state_1\", \"state_2\", \"state_3\"]]\n",
    "    action_data_ac = data[[\"action_count\"]]\n",
    "\n",
    "    X = data_ac\n",
    "    y = action_data_ac\n",
    "    #using logistic regression to fit data\n",
    "    clf = LogisticRegression(random_state=1).fit(X.values, y.values.reshape(len(y),))\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    rewards = []\n",
    "    for episode in range(100):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            #print(state)\n",
    "            #predicting\n",
    "            action = clf.predict([state])\n",
    "            next_state, reward, done, _, _ = env.step(action[0])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fitted_Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update(state, action, reward, next_state, weights, lr=0.00001):\n",
    "    pred_next = next_state.dot(weights) # evaluating Q, (n x 40) dot (40 x 2) gives (n x 2)\n",
    "    max_v = np.max(pred_next) # n x 1 where each entry is max Q value of that row\n",
    "    y = reward + 0.99*max_v # n x 1\n",
    "    temp = state.dot(weights) # n x 2\n",
    "    q = np.array([temp[i][action[i]] for i in range(len(action))]) # n x 1\n",
    "    gradient = np.reshape(q-y, (len(state), 1))*state\n",
    "    for i in range(len(state)):\n",
    "        #gradient descent\n",
    "        weights[:,action[i]] -= lr*gradient[i]\n",
    "\n",
    "def fitted_q(data, do_sigmoid = True, lr = 0.00001, k=7):\n",
    "    actions = np.array(data[:,4]).astype(int)\n",
    "    rewards = data[:,9]\n",
    "\n",
    "    #do_sigmoid = True\n",
    "    #since most of the states fall into a small subset of the bins, we use sigmoid to spread the states evenly across bins\n",
    "    if (do_sigmoid):\n",
    "        data[:,0] = np.vectorize(sigmoid)(data[:,0])\n",
    "        data[:,1] = np.vectorize(sigmoid)(data[:,1])\n",
    "        data[:,2] = np.vectorize(sigmoid)(data[:,2])\n",
    "        data[:,3] = np.vectorize(sigmoid)(data[:,3])\n",
    "\n",
    "        data[:,5] = np.vectorize(sigmoid)(data[:,5])\n",
    "        data[:,6] = np.vectorize(sigmoid)(data[:,6])\n",
    "        data[:,7] = np.vectorize(sigmoid)(data[:,7])\n",
    "        data[:,8] = np.vectorize(sigmoid)(data[:,8])\n",
    "    #discretization\n",
    "    s1 = pandas.qcut(data[:,0], 10, labels=range(0,10))\n",
    "    s2 = pandas.qcut(data[:,1], 10, labels=range(0,10))\n",
    "    s3 = pandas.qcut(data[:,2], 10, labels=range(0,10))\n",
    "    s4 = pandas.qcut(data[:,3], 10, labels=range(0,10))\n",
    "    states_onehot = np.array([one_hot(s1[i], s2[i], s3[i], s4[i]) for i in range(len(s1))]) # rows x\n",
    "\n",
    "    s1_cat = pandas.qcut(data[:,0], 10)\n",
    "    s2_cat = pandas.qcut(data[:,1], 10)\n",
    "    s3_cat = pandas.qcut(data[:,2], 10)\n",
    "    s4_cat = pandas.qcut(data[:,3], 10)\n",
    "\n",
    "    nx1 = pandas.qcut(data[:,5], 10, labels=range(0,10))\n",
    "    nx2 = pandas.qcut(data[:,6], 10, labels=range(0,10))\n",
    "    nx3 = pandas.qcut(data[:,7], 10, labels=range(0,10))\n",
    "    nx4 = pandas.qcut(data[:,8], 10, labels=range(0,10))\n",
    "    nextstates_onehot = np.array([one_hot(nx1[i], nx2[i], nx3[i], nx4[i]) for i in range(len(s1))])\n",
    "    #random initialization of weights\n",
    "    weights = np.random.uniform(-0.001, 0.001, (40,2))\n",
    "\n",
    "    #iterations\n",
    "    #k = 5\n",
    "    for i in range(k):\n",
    "        update(states_onehot, actions, rewards, nextstates_onehot, weights, lr)\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    rewards = []\n",
    "    for episode in range(100):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if (do_sigmoid):\n",
    "                state = np.vectorize(sigmoid)(state)\n",
    "            #discretizing state value received from the environment\n",
    "            b1 = getBin(state[0], s1_cat.categories)\n",
    "            b2 = getBin(state[1], s2_cat.categories)\n",
    "            b3 = getBin(state[2], s3_cat.categories)\n",
    "            b4 = getBin(state[3], s4_cat.categories)\n",
    "            state = one_hot(b1,b2,b3,b4)\n",
    "\n",
    "            action = np.argmax(state.dot(weights))\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def mlp(data):\n",
    "    data_ac = data[[\"state_0\", \"state_1\", \"state_2\", \"state_3\"]]\n",
    "    action_data_ac = data[[\"action_count\"]]\n",
    "\n",
    "    X = data_ac\n",
    "    y = action_data_ac\n",
    "    clf = MLPClassifier(random_state=1, max_iter=500).fit(X.values, y.values.reshape(len(y),))\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    rewards = []\n",
    "    for episode in range(100):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if type(state) is tuple:\n",
    "                state = state[0]\n",
    "            action = clf.predict([state])\n",
    "            next_state, reward, done, _, _ = env.step(action[0])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Comparing learning rates and iteration for fitted_Q learning\n",
    "Experiment shows learning rate of 0.00001 and iteration 7 yields best result.\n",
    "Each row represents a different learning rate\n",
    "Each column represents a different iteration (K) value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165.64 168.55 164.24]\n",
      " [166.09 170.44 165.44]\n",
      " [115.55   9.2    9.33]]\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"expert_250.csv\", delimiter=',')[1:-1]\n",
    "lrs = [0.000005, 0.00001, 0.0005]\n",
    "ks = [4,7,10]\n",
    "results = np.zeros((len(lrs),len(ks)))\n",
    "for i in range(len(lrs)):\n",
    "    for j in range(len(ks)):\n",
    "        data_test = np.copy(data)\n",
    "        mean, _ = fitted_q(data_test, True, lrs[i], ks[j])\n",
    "        results[i,j] = mean\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert_500 Simple Imitation mean:  214.71\n",
      "expert_500 Simple Imitation standard_deviation:  57.71261473889395\n",
      "expert_500 Fitted Q mean: 185.85\n",
      "expert_500 Fitted Q standard_deviation: 43.771537555813595\n",
      "expert_500 MLP mean: 333.44\n",
      "expert_500 MLP standard_deviation: 370.2914074077334\n",
      "expert_250 Simple Imitation mean:  220.72\n",
      "expert_250 Simple Imitation standard_deviation:  58.768712764531436\n",
      "expert_250 Fitted Q mean: 164.77\n",
      "expert_250 Fitted Q standard_deviation: 29.431226613921478\n",
      "expert_250 MLP mean: 312.95\n",
      "expert_250 MLP standard_deviation: 410.95961784584136\n",
      "expert_100 Simple Imitation mean:  220.66\n",
      "expert_100 Simple Imitation standard_deviation:  57.60889167481006\n",
      "expert_100 Fitted Q mean: 183.35\n",
      "expert_100 Fitted Q standard_deviation: 54.96460224544521\n"
     ]
    }
   ],
   "source": [
    "simp_means = []\n",
    "simp_stdvs = []\n",
    "fq_means = []\n",
    "fq_stdvs = []\n",
    "fq_mlp_means = []\n",
    "fq_mlp_stdvs = []\n",
    "csvs = [\"expert_500.csv\", \"expert_250.csv\",\"expert_100.csv\",\"mixed_500.csv\", \"mixed_250.csv\",\"mixed_100.csv\", \"random_500.csv\", \"random_250.csv\", \"random_100.csv\"]\n",
    "original_means = []\n",
    "for csv in csvs:\n",
    "    data = np.genfromtxt(csv, delimiter=',')[1:-1]\n",
    "    data_im = load_data(csv)\n",
    "    simp_mean, simp_stdv = simple_imitation(data_im)\n",
    "    simp_means.append(simp_mean)\n",
    "    simp_stdvs.append(simp_stdv)\n",
    "    print(f\"{csv[:-4]} Simple Imitation mean:  {simp_mean}\")\n",
    "    print(f\"{csv[:-4]} Simple Imitation standard_deviation:  {simp_stdv}\")\n",
    "    fq_mean, fq_stdv = fitted_q(data)\n",
    "    fq_means.append(fq_mean)\n",
    "    fq_stdvs.append(fq_stdv)\n",
    "    print(f\"{csv[:-4]} Fitted Q mean: {fq_mean}\")\n",
    "    print(f\"{csv[:-4]} Fitted Q standard_deviation: {fq_stdv}\")\n",
    "    fq_mlp_mean, fq_mlp_stdv = mlp(data_im)\n",
    "    fq_mlp_means.append(fq_mlp_mean)\n",
    "    fq_mlp_stdvs.append(fq_mlp_stdv)\n",
    "    print(f\"{csv[:-4]} MLP mean: {fq_mlp_mean}\")\n",
    "    print(f\"{csv[:-4]} MLP standard_deviation: {fq_mlp_stdv}\")\n",
    "    avg_mean = len(data)/float(csv[-7:-4])\n",
    "    original_means.append(avg_mean)\n",
    "\n",
    "\n",
    "# def addlabels(x,y):\n",
    "#     for i in range(len(x)):\n",
    "#         plt.text(i,y[i],y[i])\n",
    "\n",
    "n_datasets = len(csvs)\n",
    "bar_width = 0.25\n",
    "opacity = 0.8\n",
    "plt.figure(figsize = (12,6))\n",
    "index = np.arange(n_datasets)\n",
    "\n",
    "plt.bar(index, simp_means, bar_width, alpha=opacity, yerr=simp_stdvs, label=\"Imitation Learning\")\n",
    "plt.bar(index+0.2, fq_means, bar_width, alpha=opacity, yerr=fq_stdvs,\n",
    "        label=\"Fitted Q-learning (Linear)\")\n",
    "plt.bar(index+0.4, fq_mlp_means, bar_width, alpha=opacity, yerr=fq_mlp_stdvs, label=\"Fitted Q-learning (MLP)\")\n",
    "plt.bar(index+0.4, original_means, bar_width, alpha=0.4, label=\"Policy Average Return\")\n",
    "#addlabels(csvs, simp_means)\n",
    "plt.xlabel(\"Dataset and Condition\")\n",
    "plt.ylabel(\"Mean Return\")\n",
    "plt.ylim((0,300))\n",
    "plt.xticks(index + bar_width,csvs)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our experiment compares three offline RL approaches - imitation learning, an MLP, and fitted_q learning, benchmarked on gymnasium's cart_pole environment. The datasets used are generated by running expert policy for 500 episodes and random policy for 500 episodes, generating 500 episodes of half expert and half random policy data, then choose from each of them 100, 250, and 500 episodes of data to make 9 datasets.\n",
    "\n",
    "In imitation learning, we fit the data using a logistic regression model and use the fitted model to predict next action given a state value from the environment. In fitted_q learning, we use gradient descend to iteratively update the 40*2 weight table we initialized and predict the one-hot encoded state values of dimension n*40 by performing a matrix multiplication between the two. We then compute the gradient of the SSE loss and update the weights in the opposite direction of the gradient by a learning rate. We decide to train the function approximizer k = 7 iterations using learning rate of 0.00001. In our experiment, we chose k based on trial and error, but another approach to decide when to stop training is if the update in weight is smaller than some epsilon, which shows there is no significant improvement in the training.\n",
    "\n",
    "We observe that when using expert-only data, imitation learning outperforms the original data while fitted_q learning performs worse. The size of the dataset also appears to have little to no impact on the outcome of the training. We observe that although half of the episode data in the mixed datasets are from a random policy, the performance of the models are on par with models trained using only expert data. We believe the random data acts as a regularizer that prevents the model from overfitting, thus leading to better performance on unseen data. Random policy performs the worst as it has no expert input to learn any meaningful behavior.\n",
    "\n",
    "MLP performs excellently, but is beaten by immitation learning when randomness is introduced. This is likely because MLP has more capacity to fit complex funcitons, and may be erroneously fitting to the random information more closely than a logistic regression or q learning is able to. This shows that MLP requires better expert data to perform well, and is more sensitive to noise in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
